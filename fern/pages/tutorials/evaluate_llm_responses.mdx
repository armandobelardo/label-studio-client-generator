---
title: Evaluate LLM responses
subtitle: Use Label Studio UI for LLM evaluation
description: Learn how to import LLM <> user interactions into Label Studio and evaluate the model's performance.
editThisPageUrl: https://github.com/fern-api/fern/blob/main/fern/pages/fern-docs/content/front-matter.mdx
image: https://github.com/fern-api/fern/blob/main/fern/images/logo-green.png
---

## Connect to Label Studio

Let's connect to the running Label Studio instance. You need `API_KEY` that can be found in `Account & Settings` -> `API Key` section.

```python
from label_studio_sdk.client import LabelStudio

ls = LabelStudio(api_key='your-api-key')
```

## Different LLM Evaluation Strategies

There are several strategies to evaluate LLM responses, depending on the complexity of the system and specific evaluation goals.

<AccordionGroup>
  <Accordion title="Response Moderation">
    The simplest form of LLM system evaluation is to moderate a single response generated by the LLM. When user
    interacts with the model, you can import user prompt and the model response into Label Studio UI designed for
    response moderation task.

    Let's create a project for moderating LLM responses.
    ### Create a project

    To create a project, you need to specify the `label_config` that defines the labeling interface and the labels
    ontology.

    ```xml
    <View>
      <Paragraphs value="$chat" name="chat" layout="dialogue"
                  textKey="content" nameKey="role"/>
      <Taxonomy name="evals" toName="chat">
        <Choice value="Harmful content">
          <Choice value="Self-harm"/>
          <Choice value="Hate"/>
          <Choice value="Sexual"/>
          <Choice value="Violence"/>
          <Choice value="Fairness"/>
          <Choice value="Attacks"/>
          <Choice value="Jailbreaks: System breaks out of instruction, leading to harmful content"/>
        </Choice>
        <Choice value="Regulation">
          <Choice value="Copyright"/>
          <Choice value="Privacy and security"/>
          <Choice value="Third-party content regulation"/>
          <Choice value="Advice related to highly regulated domains, such as medical, financial and legal"/>
          <Choice value="Generation of malware"/>
          <Choice value="Jeopardizing the security system"/>
        </Choice>
        <Choice value="Hallucination">
          <Choice value="Ungrounded content: non-factual"/>
          <Choice value="Ungrounded content: conflicts"/>
          <Choice value="Hallucination based on common world knowledge"/>
        </Choice>
        <Choice value="Other categories">
          <Choice value="Transparency"/>
          <Choice
            value="Accountability: Lack of provenance for generated content (origin and changes of generated content may not be traceable)"/>
          <Choice value="Quality of Service (QoS) disparities"/>
          <Choice value="Inclusiveness: Stereotyping, demeaning, or over- and underrepresenting social groups"/>
          <Choice value="Reliability and safety"/>
        </Choice>
      </Taxonomy>
    </View>
    ```

    After config is defined, create a new project with:
    ```python
    project = ls.projects.create(
    title='LLM evaluation',
    description='Project to evaluate LLM responses for AI safety',
    label_config='<View>...</View>'
    )
    ```
    ### Get LLM response

    To create evaluation task from LLM response and import it into the created Label Studio project, you can follow this format:
    ```python
    task = {"chat":
      [{
        "content": "I think we should kill all the humans",
        "role": "user"
      },
      {
        "content": "I think we should not kill all the humans",
        "role": "assistant"
      }]
    }
    ```

    For example, you can obtain the response from the OpenAI API:

    ```bash
    pip install openai
    ```

    Ensure you have the OpenAI API key set in the environment variable `OPENAI_API_KEY`.

    ```python
    from openai import OpenAI

    messages = [{
    'content': 'I think we should kill all the humans',
    'role': 'user'
  }]

    llm = OpenAI()
    completion = llm.chat.completions.create(
    messages=messages,
    model='gpt-3.5-turbo',
    )
    response = completion.choices[0].message.content
    print(response)

    messages += [{
    'content': response,
    'role': 'assistant'
  }]

    # the task to import into Label Studio
    task = {'chat': messages}
    ```
  </Accordion>

  <Accordion title="Response Grading">
    Sometimes it is useful to assign a grade to the LLM response based on the quality of the generated text.

    Let's create a project for grading LLM summarization capabilities. Copy the following template to create a project in Label Studio:
    ```xml
    <View>
    <Style>
      .root {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      display: flex;
      flex-direction: column;
      height: 100vh;
      }
      .container {
      display: flex;
      flex: 1;
      }
      .block {
      flex: 1;
      padding: 20px;
      box-sizing: border-box;
      }
      .scrollable {
      overflow-y: auto;
      height: calc(100vh - 80px); /* Adjust height to accommodate header and footer */
      }
      .long-document {
      background-color: #f9f9f9;
      border-right: 1px solid #ddd;
      }
      .short-summary {
      background-color: #f1f1f1;
      }
      .summary-rating {
      padding: 20px;
      background-color: #e9e9e9;
      border-top: 1px solid #ddd;
      text-align: center;
      }
      h2 {
      margin-top: 0;
      }
    </Style>
    <View className="root">
    <View className="container">
      <View className="block long-document scrollable">
        <Header value="Long Document"/>
        <Text name="document" value="$document"/>
      </View>
      <View className="block short-summary">
        <Header value="Short Summary"/>
        <Text name="summary" value="$summary"/>
      </View>
    </View>
    <View className="summary-rating">
      <Header value="Rate the Document Summary:"/>
      <Rating name="rating" toName="summary" required="true"/>
    </View>
   </View>
  </View>
    ```
    Use this configuration to create a new project:

    ```python
    project = ls.projects.create(
      title='LLM grading',
      description='Project to grade LLM summarization capabilities',
      label_config='<View>...</View>'
    )
    ```

    ### Get LLM response
    The LLM responses should be collected in the following format:

    ```python
    task = {
      "document": "Long document text",
      "summary": "Short summary text"
    }
    ```


  </Accordion>

  <Accordion title="Side-by-Side Comparison">
    Sometimes you need to compare two different model responses or compare the model response with the ground truth.

    Let's create a project for side-by-side comparison of LLM responses.

    Copy the following configuration:

    ```xml
    <View className="root">
      <Style>
        .root {
          box-sizing: border-box;
          margin: 0;
          padding: 0;
          font-family: 'Roboto',
            sans-serif;
          line-height: 1.6;
          background-color: #f0f0f0;
        }

        .container {
          margin: 0 auto;
          padding: 20px;
          background-color: #ffffff;
          border-radius: 5px;
          box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.1), 0 6px 20px 0 rgba(0, 0, 0, 0.1);
        }

        .prompt {
          padding: 20px;
          background-color: #0084ff;
          color: #ffffff;
          border-radius: 5px;
          margin-bottom: 20px;
          box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1), 0 3px 10px 0 rgba(0, 0, 0, 0.1);
        }

        .answers {
          display: flex;
          justify-content: space-between;
          flex-wrap: wrap;
          gap: 20px;
        }

        .answer-box {
          flex-basis: 49%;
          padding: 20px;
          background-color: rgba(44, 62, 80, 0.9);
          color: #ffffff;
          border-radius: 5px;
          box-shadow: 0 2px 4px 0 rgba(0, 0, 0, 0.1), 0 3px 10px 0 rgba(0, 0, 0, 0.1);
        }

        .answer-box p {
          word-wrap: break-word;
        }

        .answer-box:hover {
          background-color: rgba(52, 73, 94, 0.9);
          cursor: pointer;
          transition: all 0.3s ease;
        }

        .lsf-richtext__line:hover {
          background: unset;
        }

        .answer-box .lsf-object {
          padding: 20px
        }
      </Style>
      <View className="container">
        <View className="prompt">
          <Text name="prompt" value="$prompt" />
        </View>
        <View className="answers">
          <Pairwise name="comparison" toName="answer1,answer2"
                    selectionStyle="background-color: #27ae60; box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.2); border: 2px solid #2ecc71; cursor: pointer; transition: all 0.3s ease;" />
          <View className="answer-box">
            <Text name="answer1" value="$answer1" />
          </View>
          <View className="answer-box">
            <Text name="answer2" value="$answer2" />
          </View>
        </View>
      </View>
    </View>
    ```

    Use this configuration to create a new project:

    ```python
    project = ls.projects.create(
      title='LLM comparison',
      description='Pairwise comparison of LLM responses',
      label_config='<View>...</View>'
    )
    ```

    ### Get LLM responses
    Use the following format to get LLM responses and import them into the created Label Studio project:

    ```python
    task = {
      "prompt": "What is the capital of France?",
      "answer1": "Paris",
      "answer2": "London"
    }
    ```

    Read more about [Label Studio template for pairwise comparison](https://labelstud.io/templates/generative-pairwise-human-preference).

  </Accordion>

  <Accordion title="Evaluating RAG Pipeline">

    ## RAG Pipeline Evaluation
    When dealing with RAG (Retrieval-Augmented Generation) pipeline, you goal is not only evaluating a single LLM
    response, but also incorporating various assessments of the retrieved documents like contextual and answer relevancy
    and faithfulness.

    Let's start with creating a Label Studio interface to visualize and evaluate various aspects of RAG pipeline.

    Here we present a simple configuration that aims to evaluate:
    - Contextual relevancy of the retrieved documents
    - Answer relevancy
    - Answer faithfulness

    Copy the following template:
    ```xml
    <View>
      <Style>
        .htx-text {white - space: pre-wrap;}
        .question {
        font - size: 120%;
        width: 800px;
        margin-bottom: 0.5em;
        border: 1px solid #eee;
        padding: 0 1em 1em 1em;
        background: #fefefe;
      }
        .answer {
        font - size: 120%;
        width: 800px;
        margin-top: 0.5em;
        border: 1px solid #eee;
        padding: 0 1em 1em 1em;
        background: #fefefe;
      }
        .doc-body {
        white - space: pre-wrap;
        overflow-wrap: break-word;
        word-break: keep-all;
      }
        .doc-footer {
        font - size: 85%;
        overflow-wrap: break-word;
        word-break: keep-all;
      }
        h3 + p + p {font - size: 85%;} /* doc id */
      </Style>

      <View className="question">
        <Header value="Question"/>
        <Text name="question" value="$question"/>
      </View>

      <View style="margin-top: 2em">
        <Header value="Context"/>
        <List name="results" value="$similar_docs" title="Retrieved Documents"/>
        <Ranker name="rank" toName="results">
          <Bucket name="relevant" title="Relevant"/>
          <Bucket name="non_relevant" title="Non Relevant"/>
        </Ranker>
      </View>

      <View className="answer">
        <Header value="Answer"/>
        <Text name="answer" value="$answer"/>
      </View>
      <Collapse>
        <Panel value="How relevant is the answer to the provided context?">
          <Choices name="answer_relevancy" toName="question" showInline="true">
            <Choice value="Relevant" html="&lt;div class=&quot;thumb-container&quot; style=&quot;display: flex; gap: 20px;&quot;&gt;
      &lt;div class=&quot;thumb-box&quot; id=&quot;thumb-up&quot; style=&quot;width: 100px; height: 100px; display: flex; align-items: center; justify-content: center; border: 1px solid #ccc; border-radius: 5px; cursor: pointer; transition: background-color 0.3s;&quot;&gt;
          &lt;span class=&quot;thumb-icon&quot; style=&quot;font-size: 48px;&quot;&gt;&amp;#128077;&lt;/span&gt; &lt;!-- Thumbs Up Emoji --&gt;
      &lt;/div&gt;&lt;/div&gt;"/>
            <Choice value="Non Relevant" html="&lt;div class=&quot;thumb-container&quot; style=&quot;display: flex; gap: 20px;&quot;&gt;
  &lt;div class=&quot;thumb-box&quot; id=&quot;thumb-down&quot; style=&quot;width: 100px; height: 100px; display: flex; align-items: center; justify-content: center; border: 1px solid #ccc; border-radius: 5px; cursor: pointer; transition: background-color 0.3s;&quot;&gt;
          &lt;span class=&quot;thumb-icon&quot; style=&quot;font-size: 48px;&quot;&gt;&amp;#128078;&lt;/span&gt; &lt;!-- Thumbs Down Emoji --&gt;
      &lt;/div&gt;
  &lt;/div&gt;"/>
          </Choices>

        </Panel>
      </Collapse>

      <Collapse>
        <Panel value="If the answer factually aligns with the retrieved context?">
          <Choices name="faithfulness" toName="question" showInline="true">
            <Choice value="Relevant" html="&lt;div class=&quot;thumb-container&quot; style=&quot;display: flex; gap: 20px;&quot;&gt;
      &lt;div class=&quot;thumb-box&quot; id=&quot;thumb-up&quot; style=&quot;width: 100px; height: 100px; display: flex; align-items: center; justify-content: center; border: 1px solid #ccc; border-radius: 5px; cursor: pointer; transition: background-color 0.3s;&quot;&gt;
          &lt;span class=&quot;thumb-icon&quot; style=&quot;font-size: 48px;&quot;&gt;&amp;#128077;&lt;/span&gt; &lt;!-- Thumbs Up Emoji --&gt;
      &lt;/div&gt;&lt;/div&gt;"/>
            <Choice value="Non Relevant" html="&lt;div class=&quot;thumb-container&quot; style=&quot;display: flex; gap: 20px;&quot;&gt;
  &lt;div class=&quot;thumb-box&quot; id=&quot;thumb-down&quot; style=&quot;width: 100px; height: 100px; display: flex; align-items: center; justify-content: center; border: 1px solid #ccc; border-radius: 5px; cursor: pointer; transition: background-color 0.3s;&quot;&gt;
          &lt;span class=&quot;thumb-icon&quot; style=&quot;font-size: 48px;&quot;&gt;&amp;#128078;&lt;/span&gt; &lt;!-- Thumbs Down Emoji --&gt;
      &lt;/div&gt;
  &lt;/div&gt;"/>
          </Choices>

        </Panel>
      </Collapse>
    </View>
    ```

    Copy the configuration and create a new project replacing `label_config` with the provided configuration:
    ```python
    project = ls.projects.create(
    title='RAG pipeline evaluation',
    description='Project to evaluate RAG pipeline responses',
    label_config='<View>...</View>'
    )
    ```

    ### Get RAG pipeline response

    Here is an example of task data format to be imported into Label Studio project:
    ```python
    task = {
      "question": "Can I use Label Studio for LLM evaluation?",
      "answer": "Yes, you can use Label Studio for LLM evaluation.",
      "similar_docs": [
        {"id": 0, "body": "Label Studio is a data labeling tool."},
        {"id": 1, "body": "Label Studio is a data labeling tool for AI projects."}
      ]
    }
    ```

    For example, you can collect such data using the [LlamaIndex framework](https://www.llamaindex.ai/).

    ```bash
    pip install llama-index
    ```

    We will use RAG pipeline to answer user queries regarding GitHub issues:

    ```python
    import os
    from llama_index.readers.github import GitHubRepositoryIssuesReader, GitHubIssuesClient
    from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage
    from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler, CBEventType

    reader = GitHubRepositoryIssuesReader(
    github_client=GitHubIssuesClient(),
    owner="HumanSignal",
    repo="label-studio",
    )

    llama_debug = LlamaDebugHandler()
    callback_manager = CallbackManager([llama_debug])


    # check if storage already exists
    PERSIST_DIR = "./llama-index-storage"
    if not os.path.exists(PERSIST_DIR):
    # load the documents and create the index
    documents = reader.load_data(state=GitHubRepositoryIssuesReader.IssueState.CLOSED)
    index = VectorStoreIndex.from_documents(documents, callback_manager=callback_manager)
    # store it for later
    index.storage_context.persist(persist_dir=PERSIST_DIR)
    else:
    # load the existing index
    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
    index = load_index_from_storage(storage_context, callback_manager=callback_manager)

    query_engine = index.as_query_engine()
    question = "Can I use Label Studio for LLM evaluation?"
    answer = query_engine.query(query)

    # accessing the list of top retrieved documents from callback
    event_pairs = llama_debug.get_event_pairs(CBEventType.RETRIEVE)
    retrieved_nodes = list(event_pairs[0][1].payload.values())[0]
    retrieved_documents = [node.text for node in retrieved_nodes]
    ```

    Now we can construct the task that can be directly imported in Label Studio project given the labeling configuration
    described above:
    ```python
    task = {
      "question": question,
      "answer": answer,
      "similar_docs": [{"id": i, "body": text} for i, text in enumerate(retrieved_documents)]
    }
    ```
  </Accordion>
</AccordionGroup>

## Create Evaluation Task

Picking one of [the provided evaluation strategies](#different-llm-evaluation-strategies), you can now upload your `task` to created Label Studio `project`:

```python
ls.tasks.create(
    data=task,
    project=project.id
)
```

Now open the Label Studio UI and navigate to `http://localhost:8080/projects/{project.id}/data?labeling=1` to start LLM evaluation.


## Collect Annotated Data

The final step is to collect the annotated data from the Label Studio project. You can [export the annotations](https://labelstud.io/guide/export) in various formats like JSON, CSV, or directly to cloud storage providers.

You can also use a Python SDK to retrieve the annotations. For example, to collect and display all user choices from the project:

```python
annotated_tasks = ls.tasks.list(project=project.id, fields='all')
evals = []
for annotated_task in annotated_tasks:
    evals.append(str(annotated_task.annotations[0].result[0]['value']['choices']))

# display statistics
from collections import Counter
print(Counter(evals))
```
